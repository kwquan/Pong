{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning \n",
    "### This notebook contains the code for creating a DQN agent for solving the environment Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install modules[IMPORTANT]\n",
    "As of the time of this writing, latest versions of gym & ale-py do not work \\\n",
    "Please install these versions to ensure pong v5 can be run successfully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym=0.24.1\n",
    "# pip install ale-py==0.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly following this paper[https://arxiv.org/abs/1312.5602] & code here[https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c] \\\n",
    "Idea is as follows:\n",
    "1) We pass 4 frames[including frame for current state] to predict state's action values \\\n",
    "2) For every episode, start by creating 3 dummy frames[consisting of state as current frame, 0s for reward & action, terminate as False] \\\n",
    "3) Using the 3 dummy frames as starting point, we pass last action[0] to obtain next frame, reward & terminate \\\n",
    "4) Continuously add them to agent's memory until a certain length is achieved \\\n",
    "5) After which, we also concurrently do weights updates by randomly selecting batch_size of frames from agent's memory for model training\n",
    "\n",
    "After looping through all episodes, save model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import gym\n",
    "import cv2\n",
    "import ale_py\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions & classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create 4 wrapper classes:\n",
    "\n",
    "1)  ProcessFrame84() \\\n",
    "&nbsp; This redefines the observation returned from each state as a grayscaled image of shape 84x84x1 \n",
    "\n",
    "2) ImageToPyTorch() \\\n",
    "&nbsp; This changes the dimension of the array(from W,H,C to C,W,H to facilitate pytorch training) \\\n",
    "\n",
    "3) BufferWrapper() \\\n",
    "&nbsp; This redefines observation returned as a stack of 4 images \\\n",
    "&nbsp; The observation method ensures return of latest 4 arrays\n",
    "\n",
    "4) ScaledFloatFrame() \\\n",
    "&nbsp; Rescales pixel values to range 0-1\n",
    "\n",
    "Finally, we add everything under function make_env() \\\n",
    "This ensures that a stack of latest 4 grayscaled images will be returned at every state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class for storing agent experience:\n",
    "ExperienceReplay() creates a deque of maxlen capacity. The sample method allows random selection of experience via randomly-picking batch_size number of indices before zipping them into their respective arrays[state, action, reward, done, new_state]\n",
    "\n",
    "Also create a namedtuple called Experience to store experience for every state before appending to the deque created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "            \n",
    "    def observation(self, obs):\n",
    "         return self.process(obs)\n",
    "\n",
    "    def process(self,frame):\n",
    "        img = frame[:, :, 0] * 0.299 + frame[:, :, 1] * 0.587 + frame[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110))\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "    \n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),dtype=np.float32)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)    \n",
    "    \n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),old_space.high.repeat(n_steps, axis=0),dtype=dtype)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer   \n",
    "    \n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0    \n",
    "    \n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name,render_mode='human',full_action_space=False)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env) \n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env) \n",
    "\n",
    "Experience = collections.namedtuple('Experience',field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "  \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer),batch_size,replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions,dtype=np.int64), np.array(rewards,dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our hyperparameters here:\n",
    "1) episodes refer to number of training episodes. Each episode terminates when either player reaches 21 points \\\n",
    "2) gamma is the decay multiplied to action-value of the next state \\\n",
    "3) min_memory_len is the minimum length of experience agent needs to attain before model training begins \\\n",
    "4) learning_rate is the learning rate used for model training \\\n",
    "5) epsilon_start is the starting value for epsilon. Recall that agent selects a random action whenever action probability falls below epsilon \\\n",
    "6) epsilon_decay is the value multiplied to epsilon. This ensures continuous decay of epsilon, resulting in agent acting more greedily as it gets better \\\n",
    "7) epsilon_min is the minimum epsilon value allowed. This ensures that agent will always carry out exploration with a very-small probability \\\n",
    "8) device stores our training data & models. Here, we are using gpu to accelerate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 800\n",
    "gamma = 0.99    \n",
    "batch_size = 32\n",
    "min_memory_len = 10000\n",
    "learning_rate = 0.0001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay = 0.99999\n",
    "epsilon_min = 0.03\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create 2 classes for function approximation & agent-training:\n",
    "1) DQN() \\\n",
    "&nbsp; This defines the neural network for estimating action values \\\n",
    "&nbsp; Largely-based on the architecture described in this paper[https://arxiv.org/abs/1312.5602]\n",
    "\n",
    "2) Agent() \\\n",
    "This defines the agent \\\n",
    "&nbsp; a) __init__() initialises parameters used for agent-training. It starts by assigning input env to env variable, creates an experience buffer called exp_buffer, resets &emsp; variable learns to 0 & calls _reset() method \\\n",
    "&nbsp; b) _reset() resets env, timestep & total_reward \\\n",
    "&nbsp; c) select_action() chooses action based on action probability. If it is < epsilon, we take a random action. Else, act greedy \\\n",
    "&nbsp; d) select_optimal_action() is used to observe agent's behavior after training. This will enable agent to take the action that maximises it's action-value at every &emsp; state \\\n",
    "&nbsp; e) get_experience() adds replay experiences to agent memory. Logic is as follows: \\\n",
    "&emsp; 1) Create episode_reward variable & set as None \\\n",
    "&emsp; 2) Latest state is passed to env.step(), along with model, epsilon & device to produce next_state, reward, terminate & info \\\n",
    "&emsp; 3) Create Experience namedtuple using self.state,action,reward,terminate,next_state \\\n",
    "&emsp; 4) Append Experience to experience buffer \\\n",
    "&emsp; 5) Set state as next_state, increment timestep & total_reward \\\n",
    "&emsp; 6) IF next state is terminal, set episode_reward as total_reward. Print termination timestep & episode_reward. Call _reset() method, return True with \\\n",
    "&emsp; episode_reward \\\n",
    "&emsp; 7) ELSE, return False & episode_reward[with None value] \\\n",
    "&emsp; 8) IF we have collected enough memory, update_weights() \\\n",
    "&nbsp; f) update_weights() is used to update model weights. Logic is as follows: \\\n",
    "&emsp; 1) Call sample method of experience buffer. Assign result as batch variable \\\n",
    "&emsp; 2) Assign resulting arrays to states, actions, rewards, dones, next_states\\\n",
    "&emsp; 3) Create tensors states_t,next_states_t, actions_t, rewards_t & done_mask \\\n",
    "&emsp; 4) Pass states_t to model to obtain action value predictions \\\n",
    "&emsp; 5) Use gather method to obtain action values for every state based on their respective actions. Call this action_values \\\n",
    "&emsp; 6) Pass next_states_t to target_model to obtain next state action values \\\n",
    "&emsp; 7) Find the max action value of each next state. Call this next_action_values \\\n",
    "&emsp; 8) Use done_mask to set terminal next state action values to 0 \\\n",
    "&emsp; 9) Calculate expected_action_values by multiplying next_action_values to gamma before adding rewards_t \\\n",
    "&emsp; 10) Calculate loss using nn.MSELoss()(action_values, expected_action_values) & do backward propagation \\\n",
    "&emsp; 11) Update optimizer & increment learns \\\n",
    "&emsp; 12) for every 1000th learns, update weights of target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Conv1 = nn.Conv2d(4,32,8,stride=4)\n",
    "        self.Conv2 = nn.Conv2d(32,64,4,stride=2)\n",
    "        self.Conv3 = nn.Conv2d(64,64,3,stride=1)\n",
    "        self.Linear1 = nn.Linear(3136,512)\n",
    "        self.Linear2 = nn.Linear(512, 6)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.Conv1(x))\n",
    "        x = F.relu(self.Conv2(x))\n",
    "        x = F.relu(self.Conv3(x))\n",
    "        x = torch.flatten(x,1,3)\n",
    "        x = F.relu(self.Linear1(x))\n",
    "        x = self.Linear2(x)\n",
    "        return x\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self,env,exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self.learns = 0\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.timestep = 0\n",
    "        self.total_reward = 0    \n",
    "            \n",
    "    def select_action(self,model,epsilon,device=device):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state = np.array([self.state], copy=False)\n",
    "            state = torch.tensor(state)\n",
    "            state = state.to(device)\n",
    "            action = np.argmax(model(state).cpu().detach().numpy())\n",
    "        return action    \n",
    "    \n",
    "    def select_optimal_action(self,model,device=device):\n",
    "        state = np.array([self.state], copy=False)\n",
    "        state = torch.tensor(state)\n",
    "        state = state.to(device)\n",
    "        action = np.argmax(model(state).cpu().detach().numpy())\n",
    "        return action \n",
    "            \n",
    "    def get_experience(self,model,target_model,epsilon,device=device):\n",
    "        episode_reward = None\n",
    "        action = self.select_action(model,epsilon,device)\n",
    "        next_state, reward, terminate, info = self.env.step(action)\n",
    "        exp = Experience(self.state,action,reward,terminate,next_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = next_state\n",
    "        self.timestep += 1\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        if terminate:\n",
    "            episode_reward = self.total_reward\n",
    "            print(f\"Score {self.timestep} timestep: {episode_reward}\")\n",
    "            self._reset()\n",
    "            return True, episode_reward\n",
    "            \n",
    "        if len(buffer) == min_memory_len:\n",
    "            self.update_weights(model,target_model)    \n",
    "\n",
    "        return False, episode_reward   \n",
    "        \n",
    "    def update_weights(self,model,target_model):\n",
    "        batch = buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "                \n",
    "        states_t = torch.tensor(states).to(device)\n",
    "        next_states_t = torch.tensor(next_states).to(device)\n",
    "        actions_t = torch.tensor(actions).to(device)\n",
    "        rewards_t = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)\n",
    "        \n",
    "        action_values = model(states_t).gather(1,actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "        next_action_values = target_model(next_states_t).max(1)[0]\n",
    "        next_action_values[done_mask] = 0.0\n",
    "        next_action_values = next_action_values.detach()\n",
    "        \n",
    "        expected_action_values = next_action_values*gamma + rewards_t\n",
    "        loss_t = nn.MSELoss()(action_values, expected_action_values)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        self.learns += 1    \n",
    "        if self.learns%1000 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "            print(f\"learns {self.learns}: target_model weights updated\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train agent\n",
    "We train for 800 episodes \\\n",
    "First, we initialise env, model & target_model \\\n",
    "Then, we create experience buffer using ExperienceReplay() \\\n",
    "Also, initialise agent, optimizer \\\n",
    "Finally, set epsilon as epislon_start & create empty list episode_rewards \\\n",
    "\n",
    "For every episode: \\\n",
    "&emsp; reset terminate as False \\\n",
    "&emsp; while not terminate, use updated epsilon value & add experience to agent's buffer \\\n",
    "&emsp; IF terminate, append reward to episode_rewards & calculate mean of latest 100 rewards[mean_reward]. Print episode & mean_reward. Print message \\\n",
    "&emsp; 'weights updated' if required memory length is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"ALE/Pong-v5\")\n",
    "net = DQN().to(device)\n",
    "target_net = copy.deepcopy(net).to(device)\n",
    "\n",
    "# net = DQN().to(device)\n",
    "# net.load_state_dict(torch.load(\"pong_agent.pth\"))\n",
    "# target_net = copy.deepcopy(net).to(device)\n",
    "buffer = ExperienceReplay(min_memory_len)\n",
    "agent = Agent(env, buffer)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "epsilon = epsilon_start\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    terminate = False\n",
    "    while not terminate:\n",
    "        epsilon = max(epsilon*epsilon_decay,epsilon_min)\n",
    "#         epsilon = epsilon_min\n",
    "        terminate, reward = agent.get_experience(net,target_net,epsilon,device=device)\n",
    "        if terminate:\n",
    "            episode_rewards.append(reward)\n",
    "            mean_reward = round(np.mean(episode_rewards[-100:]),3)\n",
    "            print(f\"episode {episode}, mean reward: {mean_reward}\")\n",
    "            if len(buffer) == min_memory_len:\n",
    "                print('weights updated')\n",
    "env.reset()        \n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights to \"pong_agent.pth\" \\\n",
    "We can load trained weights for a new agent if we do not want to retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"pong_agent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this if you only want to use pre-trained weights & observe agent in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe agent\n",
    "Let's see how our agent performs for 10 episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_net = DQN().to(device)\n",
    "trained_net.load_state_dict(torch.load(\"pong_agent.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "env = make_env(\"ALE/Pong-v5\")\n",
    "buffer = ExperienceReplay(min_memory_len)\n",
    "agent = Agent(env, buffer)\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    terminate = False\n",
    "    episode_reward = 0\n",
    "    agent._reset()\n",
    "    while not terminate:\n",
    "        action = agent.select_optimal_action(trained_net,device=device)\n",
    "        next_state, reward, terminate, info = agent.env.step(action)\n",
    "        episode_reward += reward\n",
    "        agent.state = next_state\n",
    "        if terminate:\n",
    "            episode_rewards.append(episode_reward)\n",
    "mean_reward = sum(episode_rewards)/len(episode_rewards)            \n",
    "print(\"mean reward: {%.3f}\" % mean_reward)\n",
    "\n",
    "env.reset()        \n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent was able to win most 10 games after training for 800 episodes\n",
    "One can observe certain interesting traits of the agent's behavior:\n",
    "1) Most wins are scored by hitting the ball such that the return angle cannot be reached by the opponent's paddle \\\n",
    "2) If ball starts by going towards agent's paddle at the bottom, it wins mostly by doing a \"header\"[hitting with top part of paddle to return ball at high speed, towards upper end of opponent] \\\n",
    "3) Fast returns by opponent often results in a loss \n",
    "\n",
    "There are definitely ways to improve this agent so feel free to try out your own methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
